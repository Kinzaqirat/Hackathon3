---
id: 002
title: sp-tasks breakdown for hackathon
stage: tasks
date: 2026-01-21
surface: agent
model: claude-sonnet-4-5-20250929
feature: none
branch: 002-hackathon-spec
user: pc
command: /sp.tasks
labels: [hackathon, tasks, breakdown]
links:
  spec: null
  ticket: null
  adr: null
  pr: null
files:
 - sp.tasks.md
 - tasks.md
 - history/prompts/general/002-sp-tasks.tasks.prompt.md
tests:
 - none
---

## Prompt

```text
# sp.tasks.md
# Actionable Task Breakdown - Hackathon III

This document provides granular, actionable tasks for AI agents (Claude Code and Goose) to execute. Each task is designed to be completed autonomously with minimal human intervention.

---

## Task Group 1: Environment Setup

### TASK-001: Install Development Tools
**Agent**: Human (manual installation)
**Priority**: CRITICAL
**Estimated Time**: 30 minutes

**Checklist**:
- [ ] Install Docker Desktop (macOS: `brew install --cask docker`)
- [ ] Install Minikube (`brew install minikube` or curl script)
- [ ] Install Helm (`brew install helm` or curl script)
- [ ] Install Claude Code (`brew install --cask claude-code` or curl script)
- [ ] Install Goose (`brew install --cask block-goose` or curl script)
- [ ] Install kubectl (usually comes with Docker Desktop)

**Verification**:
```bash
docker --version
minikube version
helm version
claude --version
goose --version
kubectl version --client
```

**Success Criteria**: All commands return version numbers without errors.

---

### TASK-002: Initialize Repositories
**Agent**: Claude Code or Goose
**Priority**: CRITICAL
**Estimated Time**: 10 minutes

**Prompt**:
```
Create two repositories with the following structure:

1. skills-library/
   - Initialize git repository
   - Create .claude/skills/ directory
   - Create docs/ directory
   - Create README.md with project overview

2. learnflow-app/
   - Initialize git repository
   - Create directories: frontend/, services/, infrastructure/, mcp-servers/, docs/, scripts/
   - Create symlink: ln -s ../skills-library/.claude/skills .claude/skills
   - Create README.md with application overview

Make initial commit for both repositories.
```

**Verification**:
```bash
ls skills-library/.claude/skills/
ls learnflow-app/.claude/
git log --oneline  # Both repos should have commits
```

**Success Criteria**: Both repositories initialized with correct structure.

---

### TASK-003: Start Kubernetes Cluster
**Agent**: Human (then verify with agent)
**Priority**: CRITICAL
**Estimated Time**: 15 minutes

**Commands**:
```bash
minikube start --cpus=4 --memory=8192 --driver=docker
kubectl cluster-info
kubectl get nodes
```

**Verification Prompt for Agent**:
```
Create a script that verifies the Kubernetes cluster is healthy:
- Check if nodes are Ready
- Verify kubectl can communicate with cluster
- Check available resources (CPU, memory)
- Report cluster version

Save as scripts/verify-k8s.sh and run it.
```

**Success Criteria**: Cluster running, nodes Ready, kubectl communicates successfully.

---

## Task Group 2: Foundation Skills Development

### TASK-101: Create agents-md-gen Skill
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 2 hours

**Prompt**:
```
Create a skill in .claude/skills/agents-md-gen/ that generates AGENTS.md files for repositories.

Structure:
- SKILL.md: Instructions for AI agents on when and how to use this skill
- scripts/generate_agents_md.py: Python script that analyzes a repository and generates AGENTS.md
- REFERENCE.md: Deep documentation on AGENTS.md format and best practices

Requirements:
1. SKILL.md should be ~100 tokens, concise instructions only
2. Script should:
   - Analyze directory structure
   - Detect project type (FastAPI, Next.js, Skills library, etc.)
   - Generate templated AGENTS.md with:
     - Project overview
     - Directory structure visualization
     - File naming conventions
     - Development guidelines
     - Tech stack and patterns
3. Support for multiple project types with different templates
4. REFERENCE.md should contain full templates and examples

Test the skill by generating AGENTS.md for the skills-library repository.
```

**Deliverables**:
- `.claude/skills/agents-md-gen/SKILL.md`
- `.claude/skills/agents-md-gen/scripts/generate_agents_md.py`
- `.claude/skills/agents-md-gen/REFERENCE.md`
- `skills-library/AGENTS.md` (generated by the skill)

**Verification**:
```bash
# Test with Claude Code
cd skills-library
claude "Generate AGENTS.md for this repository"

# Verify output
cat AGENTS.md
```

**Success Criteria**:
- Skill generates valid, comprehensive AGENTS.md
- Works from single prompt
- File includes all required sections

---

### TASK-102: Create mcp-code-execution Skill
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 2 hours

**Prompt**:
```
Create a skill in .claude/skills/mcp-code-execution/ that demonstrates the MCP Code Execution pattern for token efficiency.

Structure:
- SKILL.md: Explains when and how to use MCP Code Execution pattern
- scripts/mcp_client.py: Generic MCP client wrapper
- scripts/example_usage.py: Before/After comparison demonstrating token savings
- REFERENCE.md: Best practices, pattern documentation, token efficiency metrics

Requirements:
1. SKILL.md should explain:
   - When to use this pattern (large data operations, repeated calls)
   - How it reduces token usage (execution happens outside context)
   - Instructions to use scripts for MCP operations
2. mcp_client.py should:
   - Provide generic wrapper for MCP server calls
   - Handle errors gracefully
   - Return minimal results
3. example_usage.py should:
   - Show direct MCP call (baseline)
   - Show code execution pattern (optimized)
   - Calculate and display token savings
4. REFERENCE.md should include:
   - Pattern explanation from Anthropic blog
   - Token comparison table
   - Best practices for implementing pattern

Include concrete examples using filesystem or database MCP servers.
```

**Deliverables**:
- `.claude/skills/mcp-code-execution/SKILL.md`
- `.claude/skills/mcp-code-execution/scripts/mcp_client.py`
- `.claude/skills/mcp-code-execution/scripts/example_usage.py`
- `.claude/skills/mcp-code-execution/REFERENCE.md`

**Verification**:
```bash
cd skills-library/.claude/skills/mcp-code-execution
python scripts/example_usage.py

# Should output token comparison showing 80%+ reduction
```

**Success Criteria**:
- Pattern clearly documented
- Example shows significant token savings (80%+)
- Reusable client wrapper provided

---

### TASK-103: Test Cross-Agent Compatibility
**Agent**: Goose
**Priority**: HIGH
**Estimated Time**: 30 minutes

**Prompt**:
```
Test the agents-md-gen and mcp-code-execution skills using Goose:

1. Navigate to skills-library repository
2. Use agents-md-gen skill to generate AGENTS.md
3. Use mcp-code-execution skill to run example
4. Document any differences in behavior compared to Claude Code
5. Report success/failure for each skill

Create a compatibility report in docs/cross-agent-testing.md
```

**Deliverables**:
- `docs/cross-agent-testing.md` with test results
- Confirmation both skills work on Goose

**Success Criteria**: Both skills execute successfully on Goose without modifications.

---

## Task Group 3: Infrastructure Skills Development

### TASK-201: Create kafka-k8s-setup Skill
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 3 hours

**Prompt**:
```
Create a skill in .claude/skills/kafka-k8s-setup/ for deploying Apache Kafka on Kubernetes.

Structure:
- SKILL.md: Instructions for deploying Kafka
- scripts/deploy.sh: Helm installation script
- scripts/verify.py: Pod health verification
- scripts/create_topics.sh: Topic creation for LearnFlow
- REFERENCE.md: Kafka configuration options

Requirements:
1. SKILL.md should:
   - Trigger on "Deploy Kafka" or "Set up event streaming"
   - Instructions: Run deploy.sh, verify.py, create_topics.sh in sequence
   - Validation checklist
2. deploy.sh should:
   - Add Bitnami Helm repository
   - Create kafka namespace
   - Install Kafka with configuration:
     - replicaCount=1 (dev mode)
     - zookeeper.replicaCount=1
   - Output: "✓ Kafka deployed to namespace 'kafka'"
3. verify.py should:
   - Get pod status via kubectl
   - Check all pods are Running
   - Return minimal status (e.g., "✓ All 3 pods running")
   - Exit with code 0 on success, 1 on failure
4. create_topics.sh should create:
   - learning.triage, learning.concepts, learning.progress
   - code.submitted, code.review, code.debug, code.executed
   - exercise.generated, exercise.completed
   - struggle.detected, struggle.resolved
5. REFERENCE.md should include:
   - Kafka architecture overview
   - Configuration options (replicas, retention, etc.)
   - Troubleshooting common issues

Test by deploying Kafka to Minikube cluster.
```

**Deliverables**:
- `.claude/skills/kafka-k8s-setup/SKILL.md`
- `.claude/skills/kafka-k8s-setup/scripts/deploy.sh`
- `.claude/skills/kafka-k8s-setup/scripts/verify.py`
- `.claude/skills/kafka-k8s-setup/scripts/create_topics.sh`
- `.claude/skills/kafka-k8s-setup/REFERENCE.md`

**Verification**:
```bash
# Test skill
cd skills-library
claude "Deploy Kafka to Kubernetes"

# Verify deployment
kubectl get pods -n kafka
kubectl exec -it kafka-0 -n kafka -- kafka-topics.sh --list --bootstrap-server localhost:9092
```

**Success Criteria**:
- Kafka deployed successfully
- All pods Running
- All LearnFlow topics created

---

### TASK-202: Create postgres-k8s-setup Skill
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 3 hours

**Prompt**:
```
Create a skill in .claude/skills/postgres-k8s-setup/ for deploying PostgreSQL on Kubernetes.

Structure:
- SKILL.md: Instructions for deploying PostgreSQL
- scripts/deploy.sh: Helm installation with secrets
- scripts/migrate.py: Schema and migration runner
- scripts/verify.py: Connection verification
- REFERENCE.md: PostgreSQL configuration and schema docs

Requirements:
1. SKILL.md should:
   - Trigger on "Deploy PostgreSQL" or "Set up database"
   - Instructions: Run deploy.sh, migrate.py, verify.py
   - Validation checklist
2. deploy.sh should:
   - Create database namespace
   - Create Kubernetes secret for credentials
   - Install PostgreSQL via Bitnami Helm chart
   - Configuration: Single instance for dev
   - Output: "✓ PostgreSQL deployed to namespace 'database'"
3. migrate.py should:
   - Accept schema.sql file path as argument
   - Connect to PostgreSQL
   - Apply schema (CREATE TABLE statements)
   - Run migration files from migrations/ directory (if exists)
   - Support idempotent migrations
   - Return: "✓ Database initialized, N tables created"
4. verify.py should:
   - Test database connection
   - List tables created
   - Return connection string (with password masked)
5. REFERENCE.md should include:
   - LearnFlow database schema
   - Migration best practices
   - Backup/restore procedures

Include a sample schema.sql for LearnFlow with tables:
- users (id, email, password_hash, role, created_at)
- modules (id, name, description, order)
- topics (id, module_id, name, description, order)
- submissions (id, user_id, topic_id, code, result, created_at)
- quiz_results (id, user_id, topic_id, score, completed_at)
- mastery_scores (id, user_id, topic_id, score, updated_at)
- struggle_alerts (id, user_id, topic_id, reason, created_at)
- exercises (id, topic_id, title, description, difficulty)
```

**Deliverables**:
- `.claude/skills/postgres-k8s-setup/SKILL.md`
- `.claude/skills/postgres-k8s-setup/scripts/deploy.sh`
- `.claude/skills/postgres-k8s-setup/scripts/migrate.py`
- `.claude/skills/postgres-k8s-setup/scripts/verify.py`
- `.claude/skills/postgres-k8s-setup/scripts/schema.sql`
- `.claude/skills/postgres-k8s-setup/REFERENCE.md`

**Verification**:
```bash
# Test skill
cd skills-library
claude "Deploy PostgreSQL with LearnFlow schema"

# Verify
kubectl get pods -n database
kubectl exec -it postgresql-0 -n database -- psql -U learnflow -c "\dt"
```

**Success Criteria**:
- PostgreSQL deployed successfully
- Schema applied, all tables created
- Connection verification passes

---

## Task Group 4: Service Development Skills

### TASK-301: Create fastapi-dapr-agent Skill
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 4 hours

**Prompt**:
```
Create a skill in .claude/skills/fastapi-dapr-agent/ for scaffolding FastAPI microservices with Dapr and OpenAI Agents SDK.

Structure:
- SKILL.md: Instructions for creating agent services
- scripts/scaffold.py: Project generator
- scripts/dapr_config.py: Dapr component generator
- templates/: Service templates
- REFERENCE.md: Dapr patterns and best practices

Requirements:
1. SKILL.md should:
   - Trigger on "Create [name] agent service"
   - Accept parameters: service_name, agent_type, dapr_components[]
   - Instructions: Run scaffold.py, then dapr_config.py
2. scaffold.py should generate:
   - Directory structure: service_name/
   - main.py: FastAPI application
   - agent.py: OpenAI Agent implementation
   - requirements.txt: Dependencies
   - Dockerfile: Multi-stage build
   - k8s/deployment.yaml: K8s + Dapr manifests
   - k8s/service.yaml: Service definition
   - .env.example: Environment variables
3. Templates should include:
   - main.py.template:
     - FastAPI app initialization
     - Health check endpoint (/health)
     - Agent endpoint (/api/{function})
     - Dapr client integration
     - Kafka event publishing
     - State management
   - agent.py.template:
     - OpenAI client setup
     - Agent configuration
     - Tool definitions
     - Prompt templates for different agent types
   - Dockerfile.template:
     - Multi-stage: builder + runtime
     - Python 3.11+ base image
     - Optimized layer caching
   - k8s-deployment.yaml.template:
     - Deployment with Dapr annotations
     - ConfigMap for configuration
     - Environment variables
     - Resource limits
4. dapr_config.py should generate:
   - components/statestore.yaml: Redis state store
   - components/pubsub.yaml: Kafka pub/sub
   - dapr.yaml: Dapr configuration
5. REFERENCE.md should cover:
   - Dapr sidecar pattern
   - State management best practices
   - Pub/sub event-driven patterns
   - OpenAI Agents SDK usage
   - Service-to-service communication

Support for different agent types:
- Triage: Route queries to specialists
- Concepts: Explain Python concepts
- CodeReview: Analyze code quality
- Debug: Parse errors and suggest fixes
- Exercise: Generate coding challenges
- Progress: Track mastery scores

Test by creating a sample Concepts agent service.
```

**Deliverables**:
- `.claude/skills/fastapi-dapr-agent/SKILL.md`
- `.claude/skills/fastapi-dapr-agent/scripts/scaffold.py`
- `.claude/skills/fastapi-dapr-agent/scripts/dapr_config.py`
- `.claude/skills/fastapi-dapr-agent/templates/*`
- `.claude/skills/fastapi-dapr-agent/REFERENCE.md`

**Verification**:
```bash
# Test skill
cd learnflow-app
claude "Create concepts-agent service with Dapr statestore and pubsub"

# Verify structure
ls services/concepts-agent/
cat services/concepts-agent/main.py
```

**Success Criteria**:
- Service scaffolded with all components
- FastAPI app structure valid
- Dapr configuration correct
- Ready for containerization

---

### TASK-302: Generate All 6 LearnFlow Agent Services
**Agent**: Claude Code
**Priority**: HIGH
**Estimated Time**: 3 hours

**Prompt**:
```
Using the fastapi-dapr-agent skill, create all 6 LearnFlow AI agent services:

1. Triage Agent (services/triage-agent/)
   - Routes user queries to appropriate specialist agents
   - Keywords: "explain" → Concepts, "error" → Debug, "practice" → Exercise
   - Publishes to: learning.triage topic

2. Concepts Agent (services/concepts-agent/)
   - Explains Python concepts with examples
   - Adapts to student level (beginner, intermediate, advanced)
   - Publishes to: learning.concepts topic

3. Code Review Agent (services/code-review-agent/)
   - Analyzes code for correctness, style (PEP 8), efficiency
   - Provides constructive feedback
   - Publishes to: code.review topic

4. Debug Agent (services/debug-agent/)
   - Parses error messages
   - Identifies root causes
   - Provides hints before solutions
   - Publishes to: code.debug topic

5. Exercise Agent (services/exercise-agent/)
   - Generates coding challenges based on topic and difficulty
   - Auto-grades submissions
   - Publishes to: exercise.generated topic

6. Progress Agent (services/progress-agent/)
   - Tracks mastery scores by topic
   - Calculates weighted average (exercises 40%, quizzes 30%, quality 20%, streak 10%)
   - Publishes to: learning.progress topic

For each service:
- Customize agent.py with appropriate OpenAI prompts and tools
- Configure Dapr components (all use statestore + pubsub)
- Add service-specific endpoints
- Include health checks
- Generate Dockerfiles and K8s manifests

After creating all services, generate AGENTS.md for each using agents-md-gen skill.
```

**Deliverables**:
- 6 service directories under `learnflow-app/services/`
- Each with: main.py, agent.py, requirements.txt, Dockerfile, k8s/, dapr/, AGENTS.md

**Verification**:
```bash
# List all services
ls learnflow-app/services/

# Check each service structure
for service in triage concepts code-review debug exercise progress; do
  echo "Checking ${service}-agent..."
  ls learnflow-app/services/${service}-agent/
done
```

**Success Criteria**: All 6 services created with correct structure and agent-specific logic.

---

## Task Group 5: Frontend & Deployment Skills

### TASK-401: Create nextjs-k8s-deploy Skill
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 3 hours

**Prompt**:
```
Create a skill in .claude/skills/nextjs-k8s-deploy/ for building and deploying Next.js applications to Kubernetes.

Structure:
- SKILL.md: Deployment instructions
- scripts/build_docker.sh: Docker build script
- scripts/k8s_manifest.py: Manifest generator
- scripts/deploy.sh: Deployment orchestrator
- templates/Dockerfile.template: Optimized Next.js Dockerfile
- REFERENCE.md: Next.js optimization guide

Requirements:
1. SKILL.md should:
   - Trigger on "Deploy Next.js app" or "Deploy frontend"
   - Accept: app_directory, namespace, port
   - Instructions: Run build_docker.sh, k8s_manifest.py, deploy.sh
2. build_docker.sh should:
   - Use provided Dockerfile or generate from template
   - Build Docker image with tag
   - Load to Minikube or push to registry
   - Output: "✓ Image built: {image}:{tag}"
3. Dockerfile.template should:
   - Multi-stage build (dependencies → builder → runner)
   - Node 20+ Alpine base
   - Optimize layer caching
   - Production build optimizations
   - Non-root user
4. k8s_manifest.py should generate:
   - deployment.yaml: Deployment with replicas, resources
   - service.yaml: Service (NodePort for Minikube, LoadBalancer for cloud)
   - ingress.yaml (optional): Ingress for routing
   - configmap.yaml: Environment configuration
5. deploy.sh should:
   - Apply namespace
   - Apply ConfigMap
   - Apply Deployment
   - Apply Service
   - Wait for pods Ready
   - Return service URL (minikube service URL)
6. REFERENCE.md should include:
   - Next.js build optimization tips
   - Image size reduction strategies
   - Environment variable management
   - Health check configuration

Test by deploying a sample Next.js app to Minikube.
```

**Deliverables**:
- `.claude/skills/nextjs-k8s-deploy/SKILL.md`
- `.claude/skills/nextjs-k8s-deploy/scripts/build_docker.sh`
- `.claude/skills/nextjs-k8s-deploy/scripts/k8s_manifest.py`
- `.claude/skills/nextjs-k8s-deploy/scripts/deploy.sh`
- `.claude/skills/nextjs-k8s-deploy/templates/Dockerfile.template`
- `.claude/skills/nextjs-k8s-deploy/REFERENCE.md`

**Verification**:
```bash
# Test with sample app
cd learnflow-app/frontend
claude "Build and deploy this Next.js app to Kubernetes"

# Verify
kubectl get pods -n learnflow
minikube service frontend-service -n learnflow --url
```

**Success Criteria**: Next.js app built, deployed, accessible via service URL.

---

### TASK-402: Create docusaurus-deploy Skill
**Agent**: Claude Code
**Priority**: HIGH
**Estimated Time**: 2 hours

**Prompt**:
```
Create a skill in .claude/skills/docusaurus-deploy/ for generating and deploying documentation sites with Docusaurus.

Structure:
- SKILL.md: Documentation generation instructions
- scripts/init_docusaurus.sh: Scaffold Docusaurus project
- scripts/generate_docs.py: Auto-generate docs from code
- scripts/deploy.sh: Build and deploy
- REFERENCE.md: Docusaurus customization guide

Requirements:
1. SKILL.md should:
   - Trigger on "Generate documentation" or "Deploy docs"
   - Accept: project_path, output_directory
   - Instructions: Run init, generate, deploy scripts
2. init_docusaurus.sh should:
   - Check if Docusaurus exists, create if not
   - Initialize with npx create-docusaurus@latest
   - Configure docusaurus.config.js with project info
3. generate_docs.py should:
   - Parse README files from project
   - Extract docstrings from Python files
   - Extract JSDoc from TypeScript/JavaScript
   - Generate markdown files in docs/
   - Create sidebar configuration
   - Organize by project structure
4. deploy.sh should:
   - npm run build
   - Create static file server (nginx) Docker image
   - Deploy to Kubernetes or copy to static hosting
   - Return docs URL
5. REFERENCE.md should cover:
   - Docusaurus structure
   - Customization options (theme, plugins)
   - Search configuration
   - Versioning

Test by generating documentation for skills-library and learnflow-app.
```

**Deliverables**:
- `.claude/skills/docusaurus-deploy/SKILL.md`
- `.claude/skills/docusaurus-deploy/scripts/init_docusaurus.sh`
- `.claude/skills/docusaurus-deploy/scripts/generate_docs.py`
- `.claude/skills/docusaurus-deploy/scripts/deploy.sh`
- `.claude/skills/docusaurus-deploy/REFERENCE.md`

**Verification**:
```bash
# Test skill
cd learnflow-app
claude "Generate and deploy documentation for this project"

# Verify
kubectl get pods -n learnflow | grep docs
curl http://docs.learnflow.local (or minikube service URL)
```

**Success Criteria**: Documentation generated, site deployed, accessible via URL.

---

## Task Group 6: LearnFlow Application Development

### TASK-501: Develop LearnFlow Frontend
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 6 hours

**Prompt**:
```
Create the LearnFlow frontend Next.js application in learnflow-app/frontend/ with the following features:

1. Project Setup:
   - Initialize Next.js 14+ with App Router
   - Install dependencies:
     - @monaco-editor/react (code editor)
     - better-auth (authentication)
     - @tanstack/react-query (data fetching)
     - tailwindcss (styling)
     - recharts (visualizations)
   - Configure TypeScript, ESLint, Prettier

2. Authentication (Better Auth):
   - Login/Signup pages
   - JWT token management
   - Role-based access (student/teacher)
   - Protected routes

3. Student Dashboard (src/app/student/page.tsx):
   - Display user info and current module
   - Progress visualization:
     - Mastery levels by topic (color-coded: Red, Yellow, Green, Blue)
     - Completion percentage
     - Current streak
   - Quick actions: Continue Learning, Take Quiz, Practice Code

4. Teacher Dashboard (src/app/teacher/page.tsx):
   - Class overview: Number of students, average progress
   - Struggle alerts list with student names and topics
   - Analytics: Charts showing class mastery distribution
   - Action buttons: View student details, Generate exercises

5. Chat Interface (src/components/ChatInterface.tsx):
   - Message history display
   - Input field for user queries
   - Send button
   - API calls to /api/triage endpoint (via Kong Gateway)
   - Real-time response streaming
   - Message formatting (code blocks, markdown)

6. Code Editor (src/components/CodeEditor.tsx):
   - Monaco editor integration
   - Python syntax highlighting
   - Code execution button
   - Output display area
   - API call to /api/execute endpoint
   - Error display with formatting
   - Save code functionality

7. Quiz Interface (src/components/QuizInterface.tsx):
   - Display quiz questions (multiple choice and coding)
   - Answer input
   - Submit and auto-grading
   - Score display
   - Explanation of correct answers
   - Progress update after completion

8. API Client (src/lib/api.ts):
   - Fetch helper with JWT token injection
   - Endpoints for all 6 agent services
   - Error handling
   - Type definitions

9. Styling:
   - TailwindCSS configuration
   - Dark mode support
   - Responsive design (mobile-friendly)
   - Consistent color scheme

10. Environment Configuration:
    - .env.local for API URLs
    - API_BASE_URL pointing to Kong Gateway

After building, use nextjs-k8s-deploy skill to containerize and deploy to Kubernetes.
```

**Deliverables**:
- Complete Next.js application in `learnflow-app/frontend/`
- All components, pages, API client implemented
- Dockerfile and K8s manifests

**Verification**:
```bash
# Local development
cd learnflow-app/frontend
npm run dev

# Test features manually
# Then deploy
claude "Deploy this Next.js app to Kubernetes namespace learnflow"

# Verify deployment
kubectl get pods -n learnflow | grep frontend
minikube service frontend-service -n learnflow --url
```

**Success Criteria**:
- Frontend deployed and accessible
- All features functional
- API integration working

---

### TASK-502: Deploy Backend Infrastructure
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 2 hours

**Prompt**:
```
Deploy the backend infrastructure for LearnFlow to Kubernetes:

1. Use postgres-k8s-setup skill to deploy PostgreSQL:
   - Namespace: database
   - Database name: learnflow
   - Apply schema from infrastructure/postgres/schema.sql
   - Run migrations if any exist

2. Use kafka-k8s-setup skill to deploy Kafka:
   - Namespace: kafka
   - Create all LearnFlow topics:
     - learning.triage, learning.concepts, learning.progress
     - code.submitted, code.review, code.debug, code.executed
     - exercise.generated, exercise.completed
     - struggle.detected, struggle.resolved

3. Verify both deployments:
   - All PostgreSQL tables created
   - All Kafka topics available
   - Pods in Running state
   - Services accessible from other pods

Create a deployment verification script in scripts/verify-infrastructure.sh that checks all components.
```

**Deliverables**:
- PostgreSQL deployed with LearnFlow schema
- Kafka deployed with all topics
- Verification script

**Verification**:
```bash
# Run verification
./scripts/verify-infrastructure.sh

# Manual checks
kubectl get pods -n database
kubectl get pods -n kafka
kubectl exec -it kafka-0 -n kafka -- kafka-topics.sh --list --bootstrap-server localhost:9092
```

**Success Criteria**: All infrastructure deployed, verified, accessible.

---

### TASK-503: Deploy AI Agent Microservices
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 4 hours

**Prompt**:
```
Deploy all 6 LearnFlow AI agent services to Kubernetes:

For each service in services/:
1. Build Docker image:
   - docker build -t {service-name}:latest .
   - minikube image load {service-name}:latest

2. Apply Dapr components:
   - kubectl apply -f dapr/components/ -n learnflow

3. Deploy service:
   - kubectl apply -f k8s/ -n learnflow

4. Verify:
   - Pods Running
   - Dapr sidecar injected
   - Health check endpoint responding
   - Kafka connection established

Services to deploy:
- triage-agent
- concepts-agent
- code-review-agent
- debug-agent
- exercise-agent
- progress-agent

Create a deployment script in scripts/deploy-services.sh that automates this process.

After deployment, verify inter-service communication:
- Send test request to triage-agent
- Verify event published to Kafka
- Confirm concepts-agent receives and processes event
```

**Deliverables**:
- All 6 services deployed to Kubernetes
- Deployment automation script
- Inter-service communication verified

**Verification**:
```bash
# Run deployment
./scripts/deploy-services.sh

# Verify all pods
kubectl get pods -n learnflow

# Test triage agent
kubectl port-forward svc/triage-agent -n learnflow 8000:8000
curl -X POST http://localhost:8000/api/triage -d '{"query": "How do for loops work?"}'
```

**Success Criteria**: All services running, health checks passing, Kafka events flowing.

---

### TASK-504: Deploy Kong API Gateway
**Agent**: Claude Code
**Priority**: HIGH
**Estimated Time**: 2 hours

**Prompt**:
```
Deploy Kong API Gateway to route traffic to LearnFlow services:

1. Install Kong via Helm:
   - Namespace: learnflow
   - Enable Kong Ingress Controller
   - Configure database-less mode (DB-less)

2. Create Kong configuration (infrastructure/kong/kong.yaml):
   - Routes for all 6 agent services:
     - /api/triage → triage-agent:8000
     - /api/concepts → concepts-agent:8000
     - /api/review → code-review-agent:8000
     - /api/debug →
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. **Setup**: Run `.specify/scripts/powershell/check-prerequisites.ps1 -Json` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Load design documents**: Read from FEATURE_DIR:
   - **Required**: plan.md (tech stack, libraries, structure), spec.md (user stories with priorities)
   - **Optional**: data-model.md (entities), contracts/ (API endpoints), research.md (decisions), quickstart.md (test scenarios)
   - Note: Not all projects have all documents. Generate tasks based on what's available.

3. **Execute task generation workflow**:
   - Load plan.md and extract tech stack, libraries, project structure
   - Load spec.md and extract user stories with their priorities (P1, P2, P3, etc.)
   - If data-model.md exists: Extract entities and map to user stories
   - If contracts/ exists: Map endpoints to user stories
   - If research.md exists: Extract decisions for setup tasks
   - Generate tasks organized by user story (see Task Generation Rules below)
   - Generate dependency graph showing user story completion order
   - Create parallel execution examples per user story
   - Validate task completeness (each user story has all needed tasks, independently testable)

4. **Generate tasks.md**: Use `.specify/templates/tasks-template.md` as structure, fill with:
   - Correct feature name from plan.md
   - Phase 1: Setup tasks (project initialization)
   - Phase 2: Foundational tasks (blocking prerequisites for all user stories)
   - Phase 3+: One phase per user story (in priority order from spec.md)
   - Each phase includes: story goal, independent test criteria, tests (if requested), implementation tasks
   - Final Phase: Polish & cross-cutting concerns
   - All tasks must follow the strict checklist format (see Task Generation Rules below)
   - Clear file paths for each task
   - Dependencies section showing story completion order
   - Parallel execution examples per story
   - Implementation strategy section (MVP first, incremental delivery)

5. **Report**: Output path to generated tasks.md and summary:
   - Total task count
   - Task count per user story
   - Parallel opportunities identified
   - Independent test criteria for each story
   - Suggested MVP scope (typically just User Story 1)
   - Format validation: Confirm ALL tasks follow the checklist format (checkbox, ID, labels, file paths)

Context for task generation: # sp.tasks.md
# Actionable Task Breakdown - Hackathon III

This document provides granular, actionable tasks for AI agents (Claude Code and Goose) to execute. Each task is designed to be completed autonomously with minimal human intervention.

---

## Task Group 1: Environment Setup

### TASK-001: Install Development Tools
**Agent**: Human (manual installation)
**Priority**: CRITICAL
**Estimated Time**: 30 minutes

**Checklist**:
- [ ] Install Docker Desktop (macOS: `brew install --cask docker`)
- [ ] Install Minikube (`brew install minikube` or curl script)
- [ ] Install Helm (`brew install helm` or curl script)
- [ ] Install Claude Code (`brew install --cask claude-code` or curl script)
- [ ] Install Goose (`brew install --cask block-goose` or curl script)
- [ ] Install kubectl (usually comes with Docker Desktop)

**Verification**:
```bash
docker --version
minikube version
helm version
claude --version
goose --version
kubectl version --client
```

**Success Criteria**: All commands return version numbers without errors.

---

### TASK-002: Initialize Repositories
**Agent**: Claude Code or Goose
**Priority**: CRITICAL
**Estimated Time**: 10 minutes

**Prompt**:
```
Create two repositories with the following structure:

1. skills-library/
   - Initialize git repository
   - Create .claude/skills/ directory
   - Create docs/ directory
   - Create README.md with project overview

2. learnflow-app/
   - Initialize git repository
   - Create directories: frontend/, services/, infrastructure/, mcp-servers/, docs/, scripts/
   - Create symlink: ln -s ../skills-library/.claude/skills .claude/skills
   - Create README.md with application overview

Make initial commit for both repositories.
```

**Verification**:
```bash
ls skills-library/.claude/skills/
ls learnflow-app/.claude/
git log --oneline  # Both repos should have commits
```

**Success Criteria**: Both repositories initialized with correct structure.

---

### TASK-003: Start Kubernetes Cluster
**Agent**: Human (then verify with agent)
**Priority**: CRITICAL
**Estimated Time**: 15 minutes

**Commands**:
```bash
minikube start --cpus=4 --memory=8192 --driver=docker
kubectl cluster-info
kubectl get nodes
```

**Verification Prompt for Agent**:
```
Create a script that verifies the Kubernetes cluster is healthy:
- Check if nodes are Ready
- Verify kubectl can communicate with cluster
- Check available resources (CPU, memory)
- Report cluster version

Save as scripts/verify-k8s.sh and run it.
```

**Success Criteria**: Cluster running, nodes Ready, kubectl communicates successfully.

---

## Task Group 2: Foundation Skills Development

### TASK-101: Create agents-md-gen Skill
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 2 hours

**Prompt**:
```
Create a skill in .claude/skills/agents-md-gen/ that generates AGENTS.md files for repositories.

Structure:
- SKILL.md: Instructions for AI agents on when and how to use this skill
- scripts/generate_agents_md.py: Python script that analyzes a repository and generates AGENTS.md
- REFERENCE.md: Deep documentation on AGENTS.md format and best practices

Requirements:
1. SKILL.md should be ~100 tokens, concise instructions only
2. Script should:
   - Analyze directory structure
   - Detect project type (FastAPI, Next.js, Skills library, etc.)
   - Generate templated AGENTS.md with:
     - Project overview
     - Directory structure visualization
     - File naming conventions
     - Development guidelines
     - Tech stack and patterns
3. Support for multiple project types with different templates
4. REFERENCE.md should contain full templates and examples

Test the skill by generating AGENTS.md for the skills-library repository.
```

**Deliverables**:
- `.claude/skills/agents-md-gen/SKILL.md`
- `.claude/skills/agents-md-gen/scripts/generate_agents_md.py`
- `.claude/skills/agents-md-gen/REFERENCE.md`
- `skills-library/AGENTS.md` (generated by the skill)

**Verification**:
```bash
# Test with Claude Code
cd skills-library
claude "Generate AGENTS.md for this repository"

# Verify output
cat AGENTS.md
```

**Success Criteria**:
- Skill generates valid, comprehensive AGENTS.md
- Works from single prompt
- File includes all required sections

---

### TASK-102: Create mcp-code-execution Skill
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 2 hours

**Prompt**:
```
Create a skill in .claude/skills/mcp-code-execution/ that demonstrates the MCP Code Execution pattern for token efficiency.

Structure:
- SKILL.md: Explains when and how to use MCP Code Execution pattern
- scripts/mcp_client.py: Generic MCP client wrapper
- scripts/example_usage.py: Before/After comparison demonstrating token savings
- REFERENCE.md: Best practices, pattern documentation, token efficiency metrics

Requirements:
1. SKILL.md should explain:
   - When to use this pattern (large data operations, repeated calls)
   - How it reduces token usage (execution happens outside context)
   - Instructions to use scripts for MCP operations
2. mcp_client.py should:
   - Provide generic wrapper for MCP server calls
   - Handle errors gracefully
   - Return minimal results
3. example_usage.py should:
   - Show direct MCP call (baseline)
   - Show code execution pattern (optimized)
   - Calculate and display token savings
4. REFERENCE.md should include:
   - Pattern explanation from Anthropic blog
   - Token comparison table
   - Best practices for implementing pattern

Include concrete examples using filesystem or database MCP servers.
```

**Deliverables**:
- `.claude/skills/mcp-code-execution/SKILL.md`
- `.claude/skills/mcp-code-execution/scripts/mcp_client.py`
- `.claude/skills/mcp-code-execution/scripts/example_usage.py`
- `.claude/skills/mcp-code-execution/REFERENCE.md`

**Verification**:
```bash
cd skills-library/.claude/skills/mcp-code-execution
python scripts/example_usage.py

# Should output token comparison showing 80%+ reduction
```

**Success Criteria**:
- Pattern clearly documented
- Example shows significant token savings (80%+)
- Reusable client wrapper provided

---

### TASK-103: Test Cross-Agent Compatibility
**Agent**: Goose
**Priority**: HIGH
**Estimated Time**: 30 minutes

**Prompt**:
```
Test the agents-md-gen and mcp-code-execution skills using Goose:

1. Navigate to skills-library repository
2. Use agents-md-gen skill to generate AGENTS.md
3. Use mcp-code-execution skill to run example
4. Document any differences in behavior compared to Claude Code
5. Report success/failure for each skill

Create a compatibility report in docs/cross-agent-testing.md
```

**Deliverables**:
- `docs/cross-agent-testing.md` with test results
- Confirmation both skills work on Goose

**Success Criteria**: Both skills execute successfully on Goose without modifications.

---

## Task Group 3: Infrastructure Skills Development

### TASK-201: Create kafka-k8s-setup Skill
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 3 hours

**Prompt**:
```
Create a skill in .claude/skills/kafka-k8s-setup/ for deploying Apache Kafka on Kubernetes.

Structure:
- SKILL.md: Instructions for deploying Kafka
- scripts/deploy.sh: Helm installation script
- scripts/verify.py: Pod health verification
- scripts/create_topics.sh: Topic creation for LearnFlow
- REFERENCE.md: Kafka configuration options

Requirements:
1. SKILL.md should:
   - Trigger on "Deploy Kafka" or "Set up event streaming"
   - Instructions: Run deploy.sh, verify.py, create_topics.sh in sequence
   - Validation checklist
2. deploy.sh should:
   - Add Bitnami Helm repository
   - Create kafka namespace
   - Install Kafka with configuration:
     - replicaCount=1 (dev mode)
     - zookeeper.replicaCount=1
   - Output: "✓ Kafka deployed to namespace 'kafka'"
3. verify.py should:
   - Get pod status via kubectl
   - Check all pods are Running
   - Return minimal status (e.g., "✓ All 3 pods running")
   - Exit with code 0 on success, 1 on failure
4. create_topics.sh should create:
   - learning.triage, learning.concepts, learning.progress
   - code.submitted, code.review, code.debug, code.executed
   - exercise.generated, exercise.completed
   - struggle.detected, struggle.resolved
5. REFERENCE.md should include:
   - Kafka architecture overview
   - Configuration options (replicas, retention, etc.)
   - Troubleshooting common issues

Test by deploying Kafka to Minikube cluster.
```

**Deliverables**:
- `.claude/skills/kafka-k8s-setup/SKILL.md`
- `.claude/skills/kafka-k8s-setup/scripts/deploy.sh`
- `.claude/skills/kafka-k8s-setup/scripts/verify.py`
- `.claude/skills/kafka-k8s-setup/scripts/create_topics.sh`
- `.claude/skills/kafka-k8s-setup/REFERENCE.md`

**Verification**:
```bash
# Test skill
cd skills-library
claude "Deploy Kafka to Kubernetes"

# Verify deployment
kubectl get pods -n kafka
kubectl exec -it kafka-0 -n kafka -- kafka-topics.sh --list --bootstrap-server localhost:9092
```

**Success Criteria**:
- Kafka deployed successfully
- All pods Running
- All LearnFlow topics created

---

### TASK-202: Create postgres-k8s-setup Skill
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 3 hours

**Prompt**:
```
Create a skill in .claude/skills/postgres-k8s-setup/ for deploying PostgreSQL on Kubernetes.

Structure:
- SKILL.md: Instructions for deploying PostgreSQL
- scripts/deploy.sh: Helm installation with secrets
- scripts/migrate.py: Schema and migration runner
- scripts/verify.py: Connection verification
- REFERENCE.md: PostgreSQL configuration and schema docs

Requirements:
1. SKILL.md should:
   - Trigger on "Deploy PostgreSQL" or "Set up database"
   - Instructions: Run deploy.sh, migrate.py, verify.py
   - Validation checklist
2. deploy.sh should:
   - Create database namespace
   - Create Kubernetes secret for credentials
   - Install PostgreSQL via Bitnami Helm chart
   - Configuration: Single instance for dev
   - Output: "✓ PostgreSQL deployed to namespace 'database'"
3. migrate.py should:
   - Accept schema.sql file path as argument
   - Connect to PostgreSQL
   - Apply schema (CREATE TABLE statements)
   - Run migration files from migrations/ directory (if exists)
   - Support idempotent migrations
   - Return: "✓ Database initialized, N tables created"
4. verify.py should:
   - Test database connection
   - List tables created
   - Return connection string (with password masked)
5. REFERENCE.md should include:
   - LearnFlow database schema
   - Migration best practices
   - Backup/restore procedures

Include a sample schema.sql for LearnFlow with tables:
- users (id, email, password_hash, role, created_at)
- modules (id, name, description, order)
- topics (id, module_id, name, description, order)
- submissions (id, user_id, topic_id, code, result, created_at)
- quiz_results (id, user_id, topic_id, score, completed_at)
- mastery_scores (id, user_id, topic_id, score, updated_at)
- struggle_alerts (id, user_id, topic_id, reason, created_at)
- exercises (id, topic_id, title, description, difficulty)
```

**Deliverables**:
- `.claude/skills/postgres-k8s-setup/SKILL.md`
- `.claude/skills/postgres-k8s-setup/scripts/deploy.sh`
- `.claude/skills/postgres-k8s-setup/scripts/migrate.py`
- `.claude/skills/postgres-k8s-setup/scripts/verify.py`
- `.claude/skills/postgres-k8s-setup/scripts/schema.sql`
- `.claude/skills/postgres-k8s-setup/REFERENCE.md`

**Verification**:
```bash
# Test skill
cd skills-library
claude "Deploy PostgreSQL with LearnFlow schema"

# Verify
kubectl get pods -n database
kubectl exec -it postgresql-0 -n database -- psql -U learnflow -c "\dt"
```

**Success Criteria**:
- PostgreSQL deployed successfully
- Schema applied, all tables created
- Connection verification passes

---

## Task Group 4: Service Development Skills

### TASK-301: Create fastapi-dapr-agent Skill
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 4 hours

**Prompt**:
```
Create a skill in .claude/skills/fastapi-dapr-agent/ for scaffolding FastAPI microservices with Dapr and OpenAI Agents SDK.

Structure:
- SKILL.md: Instructions for creating agent services
- scripts/scaffold.py: Project generator
- scripts/dapr_config.py: Dapr component generator
- templates/: Service templates
- REFERENCE.md: Dapr patterns and best practices

Requirements:
1. SKILL.md should:
   - Trigger on "Create [name] agent service"
   - Accept parameters: service_name, agent_type, dapr_components[]
   - Instructions: Run scaffold.py, then dapr_config.py
2. scaffold.py should generate:
   - Directory structure: service_name/
   - main.py: FastAPI application
   - agent.py: OpenAI Agent implementation
   - requirements.txt: Dependencies
   - Dockerfile: Multi-stage build
   - k8s/deployment.yaml: K8s + Dapr manifests
   - k8s/service.yaml: Service definition
   - .env.example: Environment variables
3. Templates should include:
   - main.py.template:
     - FastAPI app initialization
     - Health check endpoint (/health)
     - Agent endpoint (/api/{function})
     - Dapr client integration
     - Kafka event publishing
     - State management
   - agent.py.template:
     - OpenAI client setup
     - Agent configuration
     - Tool definitions
     - Prompt templates for different agent types
   - Dockerfile.template:
     - Multi-stage: builder + runtime
     - Python 3.11+ base image
     - Optimized layer caching
   - k8s-deployment.yaml.template:
     - Deployment with Dapr annotations
     - ConfigMap for configuration
     - Environment variables
     - Resource limits
4. dapr_config.py should generate:
   - components/statestore.yaml: Redis state store
   - components/pubsub.yaml: Kafka pub/sub
   - dapr.yaml: Dapr configuration
5. REFERENCE.md should cover:
   - Dapr sidecar pattern
   - State management best practices
   - Pub/sub event-driven patterns
   - OpenAI Agents SDK usage
   - Service-to-service communication

Support for different agent types:
- Triage: Route queries to specialists
- Concepts: Explain Python concepts
- CodeReview: Analyze code quality
- Debug: Parse errors and suggest fixes
- Exercise: Generate coding challenges
- Progress: Track mastery scores

Test by creating a sample Concepts agent service.
```

**Deliverables**:
- `.claude/skills/fastapi-dapr-agent/SKILL.md`
- `.claude/skills/fastapi-dapr-agent/scripts/scaffold.py`
- `.claude/skills/fastapi-dapr-agent/scripts/dapr_config.py`
- `.claude/skills/fastapi-dapr-agent/templates/*`
- `.claude/skills/fastapi-dapr-agent/REFERENCE.md`

**Verification**:
```bash
# Test skill
cd learnflow-app
claude "Create concepts-agent service with Dapr statestore and pubsub"

# Verify structure
ls services/concepts-agent/
cat services/concepts-agent/main.py
```

**Success Criteria**:
- Service scaffolded with all components
- FastAPI app structure valid
- Dapr configuration correct
- Ready for containerization

---

### TASK-302: Generate All 6 LearnFlow Agent Services
**Agent**: Claude Code
**Priority**: HIGH
**Estimated Time**: 3 hours

**Prompt**:
```
Using the fastapi-dapr-agent skill, create all 6 LearnFlow AI agent services:

1. Triage Agent (services/triage-agent/)
   - Routes user queries to appropriate specialist agents
   - Keywords: "explain" → Concepts, "error" → Debug, "practice" → Exercise
   - Publishes to: learning.triage topic

2. Concepts Agent (services/concepts-agent/)
   - Explains Python concepts with examples
   - Adapts to student level (beginner, intermediate, advanced)
   - Publishes to: learning.concepts topic

3. Code Review Agent (services/code-review-agent/)
   - Analyzes code for correctness, style (PEP 8), efficiency
   - Provides constructive feedback
   - Publishes to: code.review topic

4. Debug Agent (services/debug-agent/)
   - Parses error messages
   - Identifies root causes
   - Provides hints before solutions
   - Publishes to: code.debug topic

5. Exercise Agent (services/exercise-agent/)
   - Generates coding challenges based on topic and difficulty
   - Auto-grades submissions
   - Publishes to: exercise.generated topic

6. Progress Agent (services/progress-agent/)
   - Tracks mastery scores by topic
   - Calculates weighted average (exercises 40%, quizzes 30%, quality 20%, streak 10%)
   - Publishes to: learning.progress topic

For each service:
- Customize agent.py with appropriate OpenAI prompts and tools
- Configure Dapr components (all use statestore + pubsub)
- Add service-specific endpoints
- Include health checks
- Generate Dockerfiles and K8s manifests

After creating all services, generate AGENTS.md for each using agents-md-gen skill.
```

**Deliverables**:
- 6 service directories under `learnflow-app/services/`
- Each with: main.py, agent.py, requirements.txt, Dockerfile, k8s/, dapr/, AGENTS.md

**Verification**:
```bash
# List all services
ls learnflow-app/services/

# Check each service structure
for service in triage concepts code-review debug exercise progress; do
  echo "Checking ${service}-agent..."
  ls learnflow-app/services/${service}-agent/
done
```

**Success Criteria**: All 6 services created with correct structure and agent-specific logic.

---

## Task Group 5: Frontend & Deployment Skills

### TASK-401: Create nextjs-k8s-deploy Skill
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 3 hours

**Prompt**:
```
Create a skill in .claude/skills/nextjs-k8s-deploy/ for building and deploying Next.js applications to Kubernetes.

Structure:
- SKILL.md: Deployment instructions
- scripts/build_docker.sh: Docker build script
- scripts/k8s_manifest.py: Manifest generator
- scripts/deploy.sh: Deployment orchestrator
- templates/Dockerfile.template: Optimized Next.js Dockerfile
- REFERENCE.md: Next.js optimization guide

Requirements:
1. SKILL.md should:
   - Trigger on "Deploy Next.js app" or "Deploy frontend"
   - Accept: app_directory, namespace, port
   - Instructions: Run build_docker.sh, k8s_manifest.py, deploy.sh
2. build_docker.sh should:
   - Use provided Dockerfile or generate from template
   - Build Docker image with tag
   - Load to Minikube or push to registry
   - Output: "✓ Image built: {image}:{tag}"
3. Dockerfile.template should:
   - Multi-stage build (dependencies → builder → runner)
   - Node 20+ Alpine base
   - Optimize layer caching
   - Production build optimizations
   - Non-root user
4. k8s_manifest.py should generate:
   - deployment.yaml: Deployment with replicas, resources
   - service.yaml: Service (NodePort for Minikube, LoadBalancer for cloud)
   - ingress.yaml (optional): Ingress for routing
   - configmap.yaml: Environment configuration
5. deploy.sh should:
   - Apply namespace
   - Apply ConfigMap
   - Apply Deployment
   - Apply Service
   - Wait for pods Ready
   - Return service URL (minikube service URL)
6. REFERENCE.md should include:
   - Next.js build optimization tips
   - Image size reduction strategies
   - Environment variable management
   - Health check configuration

Test by deploying a sample Next.js app to Minikube.
```

**Deliverables**:
- `.claude/skills/nextjs-k8s-deploy/SKILL.md`
- `.claude/skills/nextjs-k8s-deploy/scripts/build_docker.sh`
- `.claude/skills/nextjs-k8s-deploy/scripts/k8s_manifest.py`
- `.claude/skills/nextjs-k8s-deploy/scripts/deploy.sh`
- `.claude/skills/nextjs-k8s-deploy/templates/Dockerfile.template`
- `.claude/skills/nextjs-k8s-deploy/REFERENCE.md`

**Verification**:
```bash
# Test with sample app
cd learnflow-app/frontend
claude "Build and deploy this Next.js app to Kubernetes"

# Verify
kubectl get pods -n learnflow
minikube service frontend-service -n learnflow --url
```

**Success Criteria**: Next.js app built, deployed, accessible via service URL.

---

### TASK-402: Create docusaurus-deploy Skill
**Agent**: Claude Code
**Priority**: HIGH
**Estimated Time**: 2 hours

**Prompt**:
```
Create a skill in .claude/skills/docusaurus-deploy/ for generating and deploying documentation sites with Docusaurus.

Structure:
- SKILL.md: Documentation generation instructions
- scripts/init_docusaurus.sh: Scaffold Docusaurus project
- scripts/generate_docs.py: Auto-generate docs from code
- scripts/deploy.sh: Build and deploy
- REFERENCE.md: Docusaurus customization guide

Requirements:
1. SKILL.md should:
   - Trigger on "Generate documentation" or "Deploy docs"
   - Accept: project_path, output_directory
   - Instructions: Run init, generate, deploy scripts
2. init_docusaurus.sh should:
   - Check if Docusaurus exists, create if not
   - Initialize with npx create-docusaurus@latest
   - Configure docusaurus.config.js with project info
3. generate_docs.py should:
   - Parse README files from project
   - Extract docstrings from Python files
   - Extract JSDoc from TypeScript/JavaScript
   - Generate markdown files in docs/
   - Create sidebar configuration
   - Organize by project structure
4. deploy.sh should:
   - npm run build
   - Create static file server (nginx) Docker image
   - Deploy to Kubernetes or copy to static hosting
   - Return docs URL
5. REFERENCE.md should cover:
   - Docusaurus structure
   - Customization options (theme, plugins)
   - Search configuration
   - Versioning

Test by generating documentation for skills-library and learnflow-app.
```

**Deliverables**:
- `.claude/skills/docusaurus-deploy/SKILL.md`
- `.claude/skills/docusaurus-deploy/scripts/init_docusaurus.sh`
- `.claude/skills/docusaurus-deploy/scripts/generate_docs.py`
- `.claude/skills/docusaurus-deploy/scripts/deploy.sh`
- `.claude/skills/docusaurus-deploy/REFERENCE.md`

**Verification**:
```bash
# Test skill
cd learnflow-app
claude "Generate and deploy documentation for this project"

# Verify
kubectl get pods -n learnflow | grep docs
curl http://docs.learnflow.local (or minikube service URL)
```

**Success Criteria**: Documentation generated, site deployed, accessible via URL.

---

## Task Group 6: LearnFlow Application Development

### TASK-501: Develop LearnFlow Frontend
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 6 hours

**Prompt**:
```
Create the LearnFlow frontend Next.js application in learnflow-app/frontend/ with the following features:

1. Project Setup:
   - Initialize Next.js 14+ with App Router
   - Install dependencies:
     - @monaco-editor/react (code editor)
     - better-auth (authentication)
     - @tanstack/react-query (data fetching)
     - tailwindcss (styling)
     - recharts (visualizations)
   - Configure TypeScript, ESLint, Prettier

2. Authentication (Better Auth):
   - Login/Signup pages
   - JWT token management
   - Role-based access (student/teacher)
   - Protected routes

3. Student Dashboard (src/app/student/page.tsx):
   - Display user info and current module
   - Progress visualization:
     - Mastery levels by topic (color-coded: Red, Yellow, Green, Blue)
     - Completion percentage
     - Current streak
   - Quick actions: Continue Learning, Take Quiz, Practice Code

4. Teacher Dashboard (src/app/teacher/page.tsx):
   - Class overview: Number of students, average progress
   - Struggle alerts list with student names and topics
   - Analytics: Charts showing class mastery distribution
   - Action buttons: View student details, Generate exercises

5. Chat Interface (src/components/ChatInterface.tsx):
   - Message history display
   - Input field for user queries
   - Send button
   - API calls to /api/triage endpoint (via Kong Gateway)
   - Real-time response streaming
   - Message formatting (code blocks, markdown)

6. Code Editor (src/components/CodeEditor.tsx):
   - Monaco editor integration
   - Python syntax highlighting
   - Code execution button
   - Output display area
   - API call to /api/execute endpoint
   - Error display with formatting
   - Save code functionality

7. Quiz Interface (src/components/QuizInterface.tsx):
   - Display quiz questions (multiple choice and coding)
   - Answer input
   - Submit and auto-grading
   - Score display
   - Explanation of correct answers
   - Progress update after completion

8. API Client (src/lib/api.ts):
   - Fetch helper with JWT token injection
   - Endpoints for all 6 agent services
   - Error handling
   - Type definitions

9. Styling:
   - TailwindCSS configuration
   - Dark mode support
   - Responsive design (mobile-friendly)
   - Consistent color scheme

10. Environment Configuration:
    - .env.local for API URLs
    - API_BASE_URL pointing to Kong Gateway

After building, use nextjs-k8s-deploy skill to containerize and deploy to Kubernetes.
```

**Deliverables**:
- Complete Next.js application in `learnflow-app/frontend/`
- All components, pages, API client implemented
- Dockerfile and K8s manifests

**Verification**:
```bash
# Local development
cd learnflow-app/frontend
npm run dev

# Test features manually
# Then deploy
claude "Deploy this Next.js app to Kubernetes namespace learnflow"

# Verify deployment
kubectl get pods -n learnflow | grep frontend
minikube service frontend-service -n learnflow --url
```

**Success Criteria**:
- Frontend deployed and accessible
- All features functional
- API integration working

---

### TASK-502: Deploy Backend Infrastructure
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 2 hours

**Prompt**:
```
Deploy the backend infrastructure for LearnFlow to Kubernetes:

1. Use postgres-k8s-setup skill to deploy PostgreSQL:
   - Namespace: database
   - Database name: learnflow
   - Apply schema from infrastructure/postgres/schema.sql
   - Run migrations if any exist

2. Use kafka-k8s-setup skill to deploy Kafka:
   - Namespace: kafka
   - Create all LearnFlow topics:
     - learning.triage, learning.concepts, learning.progress
     - code.submitted, code.review, code.debug, code.executed
     - exercise.generated, exercise.completed
     - struggle.detected, struggle.resolved

3. Verify both deployments:
   - All PostgreSQL tables created
   - All Kafka topics available
   - Pods in Running state
   - Services accessible from other pods

Create a deployment verification script in scripts/verify-infrastructure.sh that checks all components.
```

**Deliverables**:
- PostgreSQL deployed with LearnFlow schema
- Kafka deployed with all topics
- Verification script

**Verification**:
```bash
# Run verification
./scripts/verify-infrastructure.sh

# Manual checks
kubectl get pods -n database
kubectl get pods -n kafka
kubectl exec -it kafka-0 -n kafka -- kafka-topics.sh --list --bootstrap-server localhost:9092
```

**Success Criteria**: All infrastructure deployed, verified, accessible.

---

### TASK-503: Deploy AI Agent Microservices
**Agent**: Claude Code
**Priority**: CRITICAL
**Estimated Time**: 4 hours

**Prompt**:
```
Deploy all 6 LearnFlow AI agent services to Kubernetes:

For each service in services/:
1. Build Docker image:
   - docker build -t {service-name}:latest .
   - minikube image load {service-name}:latest

2. Apply Dapr components:
   - kubectl apply -f dapr/components/ -n learnflow

3. Deploy service:
   - kubectl apply -f k8s/ -n learnflow

4. Verify:
   - Pods Running
   - Dapr sidecar injected
   - Health check endpoint responding
   - Kafka connection established

Services to deploy:
- triage-agent
- concepts-agent
- code-review-agent
- debug-agent
- exercise-agent
- progress-agent

Create a deployment script in scripts/deploy-services.sh that automates this process.

After deployment, verify inter-service communication:
- Send test request to triage-agent
- Verify event published to Kafka
- Confirm concepts-agent receives and processes event
```

**Deliverables**:
- All 6 services deployed to Kubernetes
- Deployment automation script
- Inter-service communication verified

**Verification**:
```bash
# Run deployment
./scripts/deploy-services.sh

# Verify all pods
kubectl get pods -n learnflow

# Test triage agent
kubectl port-forward svc/triage-agent -n learnflow 8000:8000
curl -X POST http://localhost:8000/api/triage -d '{"query": "How do for loops work?"}'
```

**Success Criteria**: All services running, health checks passing, Kafka events flowing.

---

### TASK-504: Deploy Kong API Gateway
**Agent**: Claude Code
**Priority**: HIGH
**Estimated Time**: 2 hours

**Prompt**:
```
Deploy Kong API Gateway to route traffic to LearnFlow services:

1. Install Kong via Helm:
   - Namespace: learnflow
   - Enable Kong Ingress Controller
   - Configure database-less mode (DB-less)

2. Create Kong configuration (infrastructure/kong/kong.yaml):
   - Routes for all 6 agent services:
     - /api/triage → triage-agent:8000
     - /api/concepts → concepts-agent:8000
     - /api/review → code-review-agent:8000
     - /api/debug →

The tasks.md should be immediately executable - each task must be specific enough that an LLM can complete it without additional context.

## Task Generation Rules

**CRITICAL**: Tasks MUST be organized by user story to enable independent implementation and testing.

**Tests are OPTIONAL**: Only generate test tasks if explicitly requested in the feature specification or if user requests TDD approach.

### Checklist Format (REQUIRED)

Every task MUST strictly follow this format:

```text
- [ ] [TaskID] [P?] [Story?] Description with file path
```

**Format Components**:

1. **Checkbox**: ALWAYS start with `- [ ]` (markdown checkbox)
2. **Task ID**: Sequential number (T001, T002, T003...) in execution order
3. **[P] marker**: Include ONLY if task is parallelizable (different files, no dependencies on incomplete tasks)
4. **[Story] label**: REQUIRED for user story phase tasks only
   - Format: [US1], [US2], [US3], etc. (maps to user stories from spec.md)
   - Setup phase: NO story label
   - Foundational phase: NO story label
   - User Story phases: MUST have story label
   - Polish phase: NO story label
5. **Description**: Clear action with exact file path

**Examples**:

- ✅ CORRECT: `- [ ] T001 Create project structure per implementation plan`
- ✅ CORRECT: `- [ ] T005 [P] Implement authentication middleware in src/middleware/auth.py`
- ✅ CORRECT: `- [ ] T012 [P] [US1] Create User model in src/models/user.py`
- ✅ CORRECT: `- [ ] T014 [US1] Implement UserService in src/services/user_service.py`
- ❌ WRONG: `- [ ] Create User model` (missing ID and Story label)
- ❌ WRONG: `T001 [US1] Create model` (missing checkbox)
- ❌ WRONG: `- [ ] [US1] Create model` (missing Task ID)
- ❌ WRONG: `- [ ] T001 [US1] Create model` (missing file path)

### Task Organization

1. **From User Stories (spec.md)** - PRIMARY ORGANIZATION:
   - Each user story (P1, P2, P3...) gets its own phase
   - Map all related components to their story:
     - Models needed for that story
     - Services needed for that story
     - Endpoints/UI needed for that story
     - If tests requested: Tests specific to that story
   - Mark story dependencies (most stories should be independent)

2. **From Contracts**:
   - Map each contract/endpoint → to the user story it serves
   - If tests requested: Each contract → contract test task [P] before implementation in that story's phase

3. **From Data Model**:
   - Map each entity to the user story(ies) that need it
   - If entity serves multiple stories: Put in earliest story or Setup phase
   - Relationships → service layer tasks in appropriate story phase

4. **From Setup/Infrastructure**:
   - Shared infrastructure → Setup phase (Phase 1)
   - Foundational/blocking tasks → Foundational phase (Phase 2)
   - Story-specific setup → within that story's phase

### Phase Structure

- **Phase 1**: Setup (project initialization)
- **Phase 2**: Foundational (blocking prerequisites - MUST complete before user stories)
- **Phase 3+**: User Stories in priority order (P1, P2, P3...)
  - Within each story: Tests (if requested) → Models → Services → Endpoints → Integration
  - Each phase should be a complete, independently testable increment
- **Final Phase**: Polish & Cross-Cutting Concerns

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record) using agent‑native tools when possible.

1) Determine Stage
   - Stage: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate Title and Determine Routing:
   - Generate Title: 3–7 words (slug for filename)
   - Route is automatically determined by stage:
     - `constitution` → `history/prompts/constitution/`
     - Feature stages → `history/prompts/<feature-name>/` (spec, plan, tasks, red, green, refactor, explainer, misc)
     - `general` → `history/prompts/general/`

3) Create and Fill PHR (Shell first; fallback agent‑native)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Open the file and fill remaining placeholders (YAML + body), embedding full PROMPT_TEXT (verbatim) and concise RESPONSE_TEXT.
   - If the script fails:
     - Read `.specify/templates/phr-template.prompt.md` (or `templates/…`)
     - Allocate an ID; compute the output path based on stage from step 2; write the file
     - Fill placeholders and embed full PROMPT_TEXT and concise RESPONSE_TEXT

4) Validate + report
   - No unresolved placeholders; path under `history/prompts/` and matches stage; stage/title/date coherent; print ID + path + stage + title.
   - On failure: warn, don't block. Skip only for `/sp.phr`.
```

## Response snapshot

I've successfully created a comprehensive tasks.md file based on the sp.tasks.md specification provided by the user. The tasks.md file contains a detailed breakdown of all the work needed for the Hackathon III project, organized into phases aligned with user stories:

1. Phase 1: Setup Tasks - Repository initialization
2. Phase 2: Foundational Tasks - Environment verification
3. Phase 3: Foundation Skills Development (US1) - Creating core skills like agents-md-gen and mcp-code-execution
4. Phase 4: Infrastructure Skills Development (US2) - Creating skills for Kafka and PostgreSQL deployment
5. Phase 5: Service Development Skills (US3) - Creating the fastapi-dapr-agent skill and generating all 6 LearnFlow agent services
6. Phase 6: Frontend & Deployment Skills (US4) - Creating deployment skills for Next.js and Docusaurus
7. Phase 7: LearnFlow Application Development (US5) - Developing the frontend and deploying backend infrastructure
8. Phase 8: Polish & Cross-Cutting Concerns - API Gateway and final verification

Each task follows the required checklist format with proper IDs, parallelization markers where appropriate, and user story labels. The tasks are granular and actionable, enabling AI agents to complete them autonomously.

I've also created a PHR (Prompt History Record) documenting this task breakdown creation in history/prompts/general/002-sp-tasks.tasks.prompt.md.

## Outcome

- ✅ Impact: Created comprehensive task breakdown for Hackathon III project
- 🧪 Tests: none
- 📁 Files: Created tasks.md and history/prompts/general/002-sp-tasks.tasks.prompt.md
- 🔁 Next prompts: Ready to begin executing the tasks in the order specified
- 🧠 Reflection: The task breakdown provides a clear roadmap for implementing the LearnFlow educational platform through skills-based development

## Evaluation notes (flywheel)

- Failure modes observed: None
- Graders run and results (PASS/FAIL): Not applicable
- Prompt variant (if applicable): Not applicable
- Next experiment (smallest change to try): Begin executing the first phase of tasks